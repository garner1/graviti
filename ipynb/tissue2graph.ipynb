{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract properties from watershed\n",
    "import glob\n",
    "import h5py\n",
    "from skimage.measure import label, regionprops\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix,lil_matrix\n",
    "import cv2\n",
    "import os\n",
    "import tensorly as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort .h5 and .tif datasets\n",
    "def custom_sorth5(name_fov): \n",
    "    [other,value]=name_fov.split('sub')\n",
    "    [value,other,other]=value.split('_')    \n",
    "    return value\n",
    "def custom_sortdapi(name_dapi): \n",
    "    [other,value]=name_dapi.split('sub')\n",
    "    [value,other]=value.split('.')    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all h5 files in a directory. \n",
    "h5dir = '/home/garner1/Work/dataset/tissue2graph/h5/'\n",
    "tifdir = '/home/garner1/Work/dataset/tissue2graph/tif/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_files = []\n",
    "for file in os.listdir(h5dir):\n",
    "    if file.endswith(\".h5\"):\n",
    "        h5_files.append(os.path.join(datadir, file))\n",
    "        \n",
    "dapi_files = []\n",
    "for file in os.listdir(tifdir):\n",
    "    if file.endswith(\".tif\"):\n",
    "        dapi_files.append(os.path.join(datadir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all tif files in directory\n",
    "dapi_files.sort(key=custom_sortdapi)\n",
    "h5_files.sort(key=custom_sorth5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the FOVs and arrange them in the position\n",
    "#To get the range we have to add one\n",
    "number_fovs=np.arange(1,len(h5_files)+1) #these are the labels of the fov\n",
    "fov_matrix=np.array(number_fovs).reshape(23,25) #reshape according to the patches 2D structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 575\n",
      "2 of 575\n"
     ]
    }
   ],
   "source": [
    "dapi_fov_counter=0 #Set counter to check current fov\n",
    "sintensity=[] #Create a list in which to save the sparses\n",
    "row_list = [] #list of rows coordinates of the fov\n",
    "col_list = [] #list of cols coordinates of the fov\n",
    "centroids = []\n",
    "\n",
    "for current_fov in h5_files[:2]:\n",
    "    print(str(dapi_fov_counter+1)+' of '+str(len(h5_files)))\n",
    "    fov = h5py.File(current_fov, 'r')\n",
    "    \n",
    "    mask=fov['/exported_watershed_masks'][:]\n",
    "    mask_reduced=np.squeeze(mask, axis=2)\n",
    "    \n",
    "    dapi_fov= cv2.imread(dapi_files[dapi_fov_counter],cv2.IMREAD_GRAYSCALE) #Get DAPI fov \n",
    "\n",
    "    #Check which position the FOV occupies within the big scan\n",
    "    #Position of FOV ilastik_masks_watershed1.h5\n",
    "    [other,value]=current_fov.split('sub')\n",
    "    [value,other,other]=value.split('_')\n",
    "    (x,y)=np.where(fov_matrix==int(value))\n",
    "    mask_label=label(mask_reduced) # label all connected components in the fov, 0 is background\n",
    "\n",
    "    row_list.append(x)\n",
    "    col_list.append(y)\n",
    "    \n",
    "    coords = [] #list of centroid coordinates for sc in each fov\n",
    "    for region in regionprops(mask_label):\n",
    "            centroid = region.centroid\n",
    "            coords.append((int(centroid[0]),int(centroid[1])))\n",
    "    centroids.append(coords)            \n",
    "#     print(x,y)\n",
    "#     plt.imshow(mask_reduced)\n",
    "#     plt.show()\n",
    "    \n",
    "    type(mask)\n",
    "    [height,width]=mask_reduced.shape\n",
    "\n",
    "    #Create a 3D sparse array where x,y are FOV size and z is the amount of nuclei in the FOV \n",
    "    sparse_in_fov=[]\n",
    "    z_dimension=0 #Set a counter to check current stack\n",
    "    for i in range(1,np.amax(mask_label)+1): # 0 is background so it doesn't get included in the range\n",
    "        xmask,ymask=np.where(mask_label==i)\n",
    "        single_cell_mask=lil_matrix((1024, 1024), dtype='uint8')\n",
    "        single_cell_mask[xmask,ymask]=dapi_fov[xmask,ymask]\n",
    "        sparse_in_fov.append(single_cell_mask) #Add current nuclei sparse on to the FOV array\n",
    "        z_dimension+=1 #Move to the next stack (next nuclei label)\n",
    "\n",
    "    sintensity.append(sparse_in_fov) #Save all sparse array in the fov\n",
    "    dapi_fov_counter+=1 #Move to the next dapi fov\n",
    "    \n",
    "#Save properties\n",
    "np.savez('fov_data.npz',sintensity=sintensity,row_list=row_list,col_list=col_list,centroids=centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fov_id = 1; cell_id = 1\n",
    "obj[fov_id][0] # row\n",
    "obj[fov_id][1] # col\n",
    "obj[fov_id][2][cell_id] # centroids\n",
    "obj[fov_id][3][cell_id] # sparse matrix of intensities\n",
    "'''\n",
    "obj = list(zip(row_list,col_list,fov_centroids,all_fov_sparses)) # obj[ind][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-41b4d0c04701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sintensity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    261\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    723\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
